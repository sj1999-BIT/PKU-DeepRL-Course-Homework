"""
full training
for now we assume the initial training is done already.

we can just load model
"""

import os.path
import re

import torch
import torch.nn as nn
import numpy as np

import gymnasium as gym

from data import append_values_to_file
from dataKeys import STATES, NEXT_STATES, ACTIONS, REWARD
from neural_network import PredictiveNetwork, PolicyNetwork, ValueNetwork, nn_learn
from tqdm import tqdm

from main import load_NN
from Ensembler import Ensembler
from dataProcessor import DataProcessor
from visual import simulate_policy

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# new update, we take into consideration possible early termination
healthy_state_range = [-100, 100]
healthy_z_range = [0.7, float('inf')]
healthy_angle_range = [-0.2, 0.2]


def check_termination(next_state_tensor):
    """
    Check if the predicted next state would cause termination
    """
    batch_size = next_state_tensor.shape[0]
    terminated = torch.zeros(batch_size, dtype=torch.bool).to(device)

    # Condition 1: Check other state elements are in healthy range
    elements_to_check = next_state_tensor[:, 1:]  # All elements except first

    if not isinstance(elements_to_check, torch.Tensor):
        elements_to_check = torch.tensor(elements_to_check, dtype=torch.float32)

    terminated |= (elements_to_check.any(dim=1) < healthy_state_range[0])
    terminated |= (elements_to_check.any(dim=1) > healthy_state_range[1])

    # Condition 2: Check height (z position)
    z_height = next_state_tensor[:, 0]
    terminated |= (z_height < healthy_z_range[0])

    # Condition 3: Check angle
    angle = next_state_tensor[:, 1]
    terminated |= (angle < healthy_angle_range[0])
    terminated |= (angle > healthy_angle_range[1])

    return terminated.float().unsqueeze(-1)

def load_NN(env: gym.Env, model_path="./"):
    if not os.path.exists(model_path):
        raise Exception(f"{model_path} path not exist, cannot load model")

    # load model

    valueNet = ValueNetwork(env)
    policyNet = PolicyNetwork(env)

    is_vNet_loaded = False
    is_pNet_loaded = False

    dynamicArr = []

    for model_weight_filename in os.listdir(model_path):
        if model_weight_filename == "Value_nn_weight.pth":
            valueNet.load_weights(filepath=os.path.join(model_path, model_weight_filename))
            is_vNet_loaded = True

        if model_weight_filename == "Policy_nn_weight.pth":
            policyNet.load_weights(filepath=os.path.join(model_path, model_weight_filename))
            is_pNet_loaded = True

        if re.match(r'^dynamics_nn_weight_(\d+)\.pth$', model_weight_filename) is not None:
            dNN = PredictiveNetwork(env)
            dNN.load_weights(filepath=os.path.join(model_path, model_weight_filename))
            dynamicArr.append(dNN)

    return policyNet, valueNet, dynamicArr, is_vNet_loaded and is_pNet_loaded



def initialise_DNN_arr(env: gym.Env, transitionProcessor: DataProcessor, ensemble_size=10,
                       epoch_num=100, batch_size=64):
    """
    Initializes and pre-trains an ensemble of Dynamic Neural Networks for model-based reinforcement learning.
    Each trajectory should be generated by the policynetwork. In this way we can have the log_probability for the
    old policyNetwork for training.

    :param ensemble_size: for boostrap ensembles, we initiate multiple dynamic nn and train them simultaneously
    :param batch_size: we generate batch of 64 for training
    :param epoch_num: total number of epoch for training
    :param env: The OpenAI Gym environment
    :param transitionProcessor: Object that handles transition data processing
    :return: Array of trained Dynamic Neural Network models
    """
    # get the state dimension and action dimension of the target environment
    # we create 10 independent DNN for bootstrap ensembles
    # for boostrap ensembles, we initiate multiple dynamic nn and train them simultaneously
    DNN_arr = [PredictiveNetwork(env, model_name=model_index) for model_index in
               range(ensemble_size)]
    ensembler = Ensembler(DNN_arr)

    path = "."

    for step in tqdm(range(epoch_num), desc=f"training {ensemble_size} dynamic NN for ensemble bootstrap"):
        sample = transitionProcessor.random_sample(batch_size=batch_size)

        # train ensembler models
        sample_arr = [dataProcessor.random_sample(batch_size=batch_size) for i in
                      range(ensembler.size)]
        ensembler.train_models(sample_arr)

    return ensembler


def calculate_gae_tensors(states, rewards, next_states, dones, value_network, gamma=0.99, lambda_=0.95):
    """
    Calculate Generalized Advantage Estimation for batched trajectories using tensors

    Args:
        states: Tensor of states [time_steps, batch_size, state_dim]
        rewards: Tensor of rewards [time_steps, batch_size]
        next_states: Tensor of next states [time_steps, batch_size, state_dim]
        dones: Tensor of done flags [time_steps, batch_size]
        value_network: Value network to estimate state values
        gamma: Discount factor (default: 0.99)
        lambda_: GAE lambda parameter (default: 0.95)

    Returns:
        advantages: Tensor of advantages [time_steps, batch_size]
        returns: Tensor of returns [time_steps, batch_size]
    """
    # Get shapes for reshaping
    batch_size = states.size(1)
    time_steps = states.size(0)
    state_dim = states.size(2)

    # Flatten the batches for efficient value network evaluation
    flat_states = states.reshape(-1, state_dim)
    flat_next_states = next_states.reshape(-1, state_dim)

    # Get value estimates (single batch processing)
    with torch.no_grad():
        flat_values = value_network.forward(flat_states)
        flat_next_values = value_network.forward(flat_next_states)

    # Reshape values back to [time_steps, batch_size]
    values = flat_values.reshape(time_steps, batch_size).to(device)
    next_values = flat_next_values.reshape(time_steps, batch_size).to(device)

    # Ensure rewards and dones have the right shape
    if rewards.dim() == 3:  # If rewards has shape [time_steps, batch_size, 1]
        rewards = rewards.squeeze(-1)
    if dones.dim() == 3:  # If dones has shape [time_steps, batch_size, 1]
        dones = dones.squeeze(-1)

    # Calculate td-error (δₜ = rₜ + γV(s_{t+1}) - V(sₜ))
    deltas = rewards + gamma * next_values * (~dones.bool()).float() - values

    # Initialize advantages tensor
    advantages = torch.zeros_like(deltas).to(device)

    # Calculate GAE by working backwards
    last_gae = torch.zeros(batch_size, device=states.device)

    for t in reversed(range(time_steps)):
        # For done episodes, reset the last_gae
        mask = (~dones.bool()[t]).float()
        last_gae = deltas[t] + gamma * lambda_ * mask * last_gae
        advantages[t] = last_gae

    # Calculate returns (G = A + V)
    returns = advantages + values

    # Normalize advantages
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    return advantages, returns


def rollout(state_distribution: torch.distributions.normal.Normal, ensembler: Ensembler, policyNet: PolicyNetwork,
            valueNet: ValueNetwork, rollout_size=400, trajectory_len=1):
    """
    Generates training data by collecting trajectories using the provided policy network,
    storing all data in tensor form for faster processing.

    This method runs the provided policy in multiple environments simultaneously, collecting
    state transitions, actions, rewards, and other relevant information. It then calculates
    advantages and returns using GAE for policy optimization.

    Args:
        envs: List of gym environments to collect trajectories from
        policyNet: The policy network used to select actions
        valueNet: The value network for GAE calculation

    Returns:
        A dictionary containing tensors for states, actions, rewards, old_log_probs,
        advantages, returns, and other trajectory data
    """

    # Initialize tensor lists for current batch
    batch_states = []
    batch_actions = []
    batch_rewards = []
    batch_old_log_probs = []
    batch_next_states = []
    batch_dones = []

    state_distribution = state_distribution

    # Initialize environment states
    current_states_tensor_arr = []

    for _ in range(rollout_size):
        # sample a random state
        state_tensor = state_distribution.sample()
        current_states_tensor_arr.append(state_tensor)

    with torch.no_grad():

        # Convert current states to tensor
        states_tensor = torch.stack(current_states_tensor_arr).to(device)

        # Process steps for fixed episode length
        for trajectory_step in tqdm(range(trajectory_len), desc="Collecting rollout experience"):

            # Get actions using policy network (stay in tensor form)
            actions_tensor, log_probs_tensor = policyNet.get_action(states_tensor)

            # Store current states
            batch_states.append(states_tensor)
            batch_actions.append(actions_tensor)
            batch_old_log_probs.append(log_probs_tensor)

            # Process environment steps
            new_states = []
            rewards = []

            # Step through environments (still need numpy for gym)
            for state, action in zip(states_tensor.cpu().numpy(), actions_tensor.cpu().numpy()):
                # Step environment
                next_state, reward, _, _ = ensembler.get_random_next_state_and_reward(state, action)

                # Store results
                new_states.append(next_state)
                rewards.append(reward)

            # Convert new data to tensors
            rewards_tensor = torch.FloatTensor(rewards).to(device)
            next_states_tensor = torch.stack(new_states).to(device)
            dones_tensor = check_termination(next_states_tensor)


            # Store step results
            batch_rewards.append(rewards_tensor)
            batch_next_states.append(next_states_tensor)
            batch_dones.append(dones_tensor)

            # Update for next iteration
            states_tensor = next_states_tensor

            # Break if all environments are done
            if not dones_tensor.any() > 0:
                break

        # Stack tensors along time dimension (making each tensor shape [time_steps, batch_size, ...])
        trajectory_tensors = {
            'states': torch.stack(batch_states),
            'actions': torch.stack(batch_actions),
            'rewards': torch.stack(batch_rewards),
            'old_log_probs': torch.stack(batch_old_log_probs),
            'next_states': torch.stack(batch_next_states),
            'dones': torch.stack(batch_dones)
        }

        # Calculate advantages and returns as tensors
        advantages, returns = calculate_gae_tensors(
            trajectory_tensors['states'],
            trajectory_tensors['rewards'],
            trajectory_tensors['next_states'],
            trajectory_tensors['dones'],
            valueNet
        )

        # Add to trajectory tensors
        trajectory_tensors['advantages'] = advantages
        trajectory_tensors['returns'] = returns

        # need to rearrange the tensor for sampling
        for key in trajectory_tensors.keys():
            if trajectory_tensors[key].dim() == 3:
                trajectory_tensors[key] = trajectory_tensors[key].permute(1, 0, 2)
            if trajectory_tensors[key].dim() == 2:
                trajectory_tensors[key] = trajectory_tensors[key].permute(1, 0)

        for key, tensor in trajectory_tensors.items():
            if len(tensor.shape) == 2:
                # Handle (N, 2) case - flatten to (N*2)
                trajectory_tensors[key] = tensor.reshape(-1)
            elif len(tensor.shape) == 3:
                # Handle (N, 2, F) case - reshape to (N*2, F)
                batch_size, pairs, features = tensor.shape
                trajectory_tensors[key] = tensor.reshape(batch_size * pairs, features)
            else:
                # Keep other tensors unchanged
                trajectory_tensors[key] = tensor

        return trajectory_tensors


def single_PPO_update(policy_net: PolicyNetwork, value_net: ValueNetwork, trajectory_tensor,
                      data_processor: DataProcessor,
                      batch_size=64, epoch_num=40):
    """
    In each epoch we aim the following things
    1. collect data using the policy network and the replayBuffer
    2. random sample training batches to train both network till convergence.
    :param batch_size: batch size used from random sample to train network
    :param envs: initiated environments for simulation
    :param data_processor:
    :param policy_net:
    :param value_net:
    :return:
    """

    """
    collect data for using policy and value network with the initiated envs
    The trajectory tensors contain keys:

    states: Environment states at each timestep, initially shaped [timesteps, trajectories, state_dim]
    actions: Actions taken by the policy, initially shaped [timesteps, trajectories, action_dim]
    rewards: Rewards received after each action, initially shaped [timesteps, trajectories]
    old_probs: Action probabilities from the policy that generated the data, initially shaped [timesteps, trajectories]
    next_states: States after taking actions, initially shaped [timesteps, trajectories, state_dim]
    dones: Boolean flags indicating terminal states, initially shaped [timesteps, trajectories]
    advantages: Computed advantage estimates using GAE, likely shaped [timesteps, trajectories]
    returns: Computed returns (discounted sum of rewards), likely shaped [timesteps, trajectories]
    """

    policy_loss_arr = []
    value_loss_arr = []

    for _ in range(epoch_num):
        # extract a random batch of data for training.
        batch_training_data_tensor = data_processor.random_sample(trajectory_tensor, batch_size=batch_size)

        # policy network learning
        policy_loss = policy_net.get_loss(batch_training_data_tensor)
        policy_net.optimizer.zero_grad()
        policy_loss.backward()
        policy_net.optimizer.step()

        # value network learning
        value_loss = value_net.get_loss(batch_training_data_tensor)
        value_net.optimizer.zero_grad()
        value_loss.backward()
        value_net.optimizer.step()

        # save the loss value for tracking
        policy_loss_arr.append(policy_loss.detach().item())
        value_loss_arr.append(value_loss.detach().item())

    append_values_to_file(policy_loss_arr, "./data/policy_loss.txt")
    # plot_progress_data(policy_loss_arr, save_plot=True, plot_file_title="policy_loss")

    append_values_to_file(value_loss_arr, "./data/value_loss.txt")
    # plot_progress_data(value_loss_arr, save_plot=True, plot_file_title="value_loss")


if __name__ == "__main__":
    path = "."
    test_env = gym.make("Hopper-v5", render_mode=None)

    policyNet, valueNet, dynamicArr, is_initialised = load_NN(test_env, path)
    dataProcessor = DataProcessor(test_env, policyNet, need_initialise=True)

    # no initial weights, we need to train them
    if not is_initialised:
        # no weights, need prepare
        ensemble_predictor = initialise_DNN_arr(test_env, dataProcessor, ensemble_size=10,
                                                epoch_num=100, batch_size=64)
        # save current policy weights and valueNet weights
        policyNet.save_weights()
        valueNet.save_weights()
    else:
        # create ensembler with the list of dynamic NN
        ensemble_predictor = Ensembler(dynamicArr)

    # training
    epoch_time = 0
    single_update_epoch_num = 40
    batch_size = 64
    training_num = 300
    epoch_training_num = 100

    # should increase overtime, up to 15
    horizon = 0
    horizon_increment_interval = int(training_num / 15)

    for training_step in range(training_num):
        print(f"############### current training step {training_step}")

        # need to maintain cur_state for rollout
        cur_state, _ = test_env.reset()

        cur_states = []
        next_states = []
        rewards = []
        actions = []

        if training_step % horizon_increment_interval == 0:
            horizon += 1

        for step in range(epoch_training_num):
            # we run rollout
            # first get the numpy action from policy
            policy_action_tensor, log_prob_tensor = policyNet.get_action(cur_state)

            policy_action = policy_action_tensor.cpu().numpy()

            # Step environment
            next_state, reward, terminated, truncated, _ = test_env.step(policy_action)

            # collect the environment data
            cur_states.append(torch.tensor(cur_state, dtype=torch.float32))
            next_states.append(torch.tensor(next_state, dtype=torch.float32))
            rewards.append(torch.tensor(reward, dtype=torch.float32))
            actions.append(torch.tensor(policy_action, dtype=torch.float32))

            # conduct rollout out
            # first get the next state distribution
            _, _, next_state_distribution, _ = ensemble_predictor.get_random_next_state_and_reward(cur_state,
                                                                                                   policy_action)
            generated_data = rollout(next_state_distribution, ensemble_predictor, policyNet, valueNet, rollout_size=400,
                                     trajectory_len=horizon)

            # train the PPO with the generated model
            single_PPO_update(policyNet, valueNet, generated_data, dataProcessor, batch_size=batch_size,
                              epoch_num=single_update_epoch_num)

            # get reward to see how well the model currently preforms
            reward = simulate_policy()
            append_values_to_file(reward, filename="reward.txt")

        result = {
            STATES: torch.stack(cur_states),
            NEXT_STATES: torch.stack(next_states),
            REWARD: torch.stack(rewards),
            ACTIONS: torch.stack(actions).to(device)
        }

        # all environment data
        dataProcessor.update_transition_buffer(result)

        # train the ensmbler with the updated environment data
        for step in tqdm(range(single_update_epoch_num), desc=f"training value network and dynamic model"):
            # train ensembler models
            batch_dynamic_sample_arr = [dataProcessor.random_sample(batch_size=batch_size) for i in
                                        range(ensemble_predictor.size)]
            ensemble_predictor.train_models(batch_dynamic_sample_arr)
