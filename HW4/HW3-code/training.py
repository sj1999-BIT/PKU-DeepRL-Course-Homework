"""
full training
for now we assume the initial training is done already.

we can just load model
"""

import os.path

import torch
import torch.nn as nn

import gymnasium as gym

from data import append_values_to_file
from neural_network import DynamicsNetwork, PolicyNetwork, ValueNetwork, nn_learn
from tqdm import tqdm

from main import load_NN
from Ensembler import Ensembler
from dataProcessor import DataProcessor
from visual import simulate_policy


def initialise_DNN_arr_and_val(env: gym.Env, transitionProcessor: DataProcessor, ensemble_size=10,
                               epoch_num=100, batch_size=64):
    """
    Initializes and pre-trains an ensemble of Dynamic Neural Networks for model-based reinforcement learning.
    Each trajectory should be generated by the policynetwork. In this way we can have the log_probability for the
    old policyNetwork for training.

    :param ensemble_size: for boostrap ensembles, we initiate multiple dynamic nn and train them simultaneously
    :param batch_size: we generate batch of 64 for training
    :param epoch_num: total number of epoch for training
    :param env: The OpenAI Gym environment
    :param transitionProcessor: Object that handles transition data processing
    :return: Array of trained Dynamic Neural Network models
    """
    # get the state dimension and action dimension of the target environment
    # we create 10 independent DNN for bootstrap ensembles
    # for boostrap ensembles, we initiate multiple dynamic nn and train them simultaneously
    DNN_arr = [DynamicsNetwork(env, model_name=model_index) for model_index in
               range(ensemble_size)]
    ensembler = Ensembler(DNN_arr)

    vNet = ValueNetwork(env)

    path = "."

    for step in tqdm(range(epoch_num), desc=f"training {ensemble_size} dynamic NN for ensemble bootstrap"):
        sample = transitionProcessor.random_sample(batch_size=batch_size)
        nn_learn(valueNet, sample, filepath=f"./value_loss.txt")

        # train ensembler models
        sample_arr = [dataProcessor.random_sample(batch_size=batch_size) for i in
                      range(ensembler.size)]
        ensembler.train_models(sample_arr)

    return ensembler, vNet


if __name__ == "__main__":
    path = "."
    test_env = gym.make("Hopper-v5", render_mode=None)

    policyNet, valueNet, dynamicArr, is_initialised = load_NN(test_env, path)
    dataProcessor = DataProcessor(test_env, policyNet, need_initialise=True)

    # no initial weights, we need to train them
    if not is_initialised:
        # no weights, need prepare
        ensemble_dynamic, valueNet = initialise_DNN_arr_and_val(test_env, dataProcessor, ensemble_size=10,
                                                                epoch_num=100, batch_size=64)
        # save current policy weights
        policyNet.save_weights()
    else:
        # create ensembler with the list of dynamic NN
        ensemble_dynamic = Ensembler(dynamicArr)

    # training
    epoch_time = 0
    single_update_epoch_num = 20
    batch_size = 64

    while True:
        epoch_time += 1

        # collect close loop data using MPC
        policy_training_data = dataProcessor.generate_policy_training_data(policyNet, valueNet, ensemble_dynamic,
                                                                           trajectory_num=1000, trajectory_len=100,
                                                                           discount_factor=0.9, time_step_num=1000)

        # train all the models
        for step in tqdm(range(single_update_epoch_num), desc=f"training value network"):
            # train value net
            batch_value_sample = dataProcessor.random_sample(batch_size=batch_size)
            nn_learn(valueNet, batch_value_sample, filepath=f"./value_loss.txt")

            # train policy net
            batch_policy_sample = dataProcessor.random_sample(tensors_dict=policy_training_data, batch_size=batch_size)
            nn_learn(policyNet, batch_policy_sample, filepath=f"./policy_loss.txt")

            # train ensembler models
            batch_dynamic_sample_arr = [dataProcessor.random_sample(batch_size=batch_size) for i in
                                        range(ensemble_dynamic.size)]
            ensemble_dynamic.train_models(batch_dynamic_sample_arr)

        # get reward to see how well the model currently preforms
        reward = simulate_policy()
        append_values_to_file(reward, filename="reward.txt")
